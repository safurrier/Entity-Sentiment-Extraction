{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_CONFIG_ERR_MSG = \"\"\"No config file found. Root directory is determined by presence of \"config.yaml\" file.\"\"\"        \n",
    "\n",
    "original_wd = os.getcwd()\n",
    "\n",
    "# Number of times to move back in directory\n",
    "num_retries = 10\n",
    "for x in range(0, num_retries):\n",
    "    # try to load config file    \n",
    "    try:\n",
    "        with open(\"config.yaml\", 'r') as stream:\n",
    "            cfg = yaml.safe_load(stream)\n",
    "    # If not found move back one directory level\n",
    "    except FileNotFoundError:\n",
    "        os.chdir('../')\n",
    "        # If reached the max number of directory levels change to original wd and print error msg\n",
    "        if x+1 == num_retries:\n",
    "            os.chdir(original_wd)\n",
    "            print(NO_CONFIG_ERR_MSG)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import local code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Add current wd to path for localimports\n",
    "path = os.getcwd()\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path) \n",
    "\n",
    "from src.convenience_functions.textacy_convenience_functions import load_textacy_corpus\n",
    "from src.convenience_functions.textacy_convenience_functions import entity_statements\n",
    "from src.convenience_functions.textacy_convenience_functions import list_of_entity_statements\n",
    "from src.convenience_functions.textacy_convenience_functions import dask_df_apply\n",
    "from src.textblob_entity_sentiment import textblob_entity_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\")\n",
    "logging.basicConfig(filename='logs/{}.txt'.format(now), \n",
    "                    level=logging.INFO,\n",
    "                    filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Reading in data from {}\"\"\".format(cfg['input_filepath']))\n",
    "\n",
    "\n",
    "df = pd.read_csv(cfg['input_filepath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Multiprocessing of applied textacy docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dask to multiprocess the loading of textacy docs for each text\n",
    "\n",
    "1. Use dask to create partitioned dataframe\n",
    "\n",
    "2. To each partition map an apply that creates textacy docs from the Policy_Text column\n",
    "\n",
    "3. Concatenate back to original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Creating textacy Doc objects using the text found in the '{}' column\"\"\".format(cfg['text_col']))\n",
    "\n",
    "df = dask_df_apply(df, cfg['text_col'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_entity_sentiment(df,\n",
    "                              textacy_col,\n",
    "                              entity,\n",
    "                              inplace=True,\n",
    "                              subjectivity=False,\n",
    "                              keep_stats=['count', 'mean', 'min', '25%', '50%', '75%', 'max']):\n",
    "    \"\"\"\n",
    "    Pull the descriptive sentiment stats of text sentence with a specified entity in it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataframe which holds the text\n",
    "    textacy_col : str\n",
    "        The name to give to the column with the textacy doc objects\n",
    "    entity : str\n",
    "        The entity to search the textacy Doc object for\n",
    "    inplace : bool\n",
    "        Whether to return the entire df with the sentiment info or the sentiment info alone\n",
    "        Default is False\n",
    "    subjectivity : bool\n",
    "        Whether to include the subjectivity of the sentiment. Defaults to False.\n",
    "    keep_stats : list\n",
    "        A list of the summary statistics to keep. Default is all returned by pandas DataFrame.describe() method\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Either the dataframe passed as arg with the sentiment info as trailing columns\n",
    "        or the sentiment descriptive stats by itself\n",
    "    \"\"\"\n",
    "    sentiment_rows = []\n",
    "    for text in df[textacy_col].values:\n",
    "        text_entities = list(entity_statements(text, entity))\n",
    "\n",
    "         # Iterate through all sentences and get sentiment analysis\n",
    "        entity_sentiment_info = [textblob.TextBlob(sentence).sentiment_assessments\n",
    "                                for\n",
    "                                sentence\n",
    "                                in\n",
    "                                text_entities]\n",
    "\n",
    "        # After taking sentiments, turn into a dataframe and describe\n",
    "        try:\n",
    "            # Indices and columns to keep\n",
    "            #keep_stats = ['count', 'mean', 'min', '25%', '50%', '75%', 'max']\n",
    "            keep_cols = ['polarity']\n",
    "\n",
    "            # If subjectivity is set to true, values for it will also be captured\n",
    "            if subjectivity:\n",
    "                keep_cols.append('subjectivity')\n",
    "\n",
    "            # Describe those columns\n",
    "            summary_stats = pd.DataFrame(entity_sentiment_info).describe().loc[keep_stats, keep_cols]\n",
    "\n",
    "            # Add row to list\n",
    "            sentiment_rows.append(pivot_df_to_row(summary_stats))\n",
    "\n",
    "        # If there's nothing to describe\n",
    "        except ValueError as e:\n",
    "            # Create a summary stats with nulls\n",
    "            summary_stats = pd.DataFrame(index=keep_stats, columns=keep_cols)\n",
    "\n",
    "            # Add to list of rows\n",
    "            sentiment_rows.append(pivot_df_to_row(summary_stats))\n",
    "    # Concatenate All rows together into one dataframe\n",
    "    sentiment_df = pd.concat(sentiment_rows).add_prefix(entity+'_')\n",
    "\n",
    "    if not inplace:\n",
    "        return sentiment_df.reset_index(drop=True)\n",
    "    else:\n",
    "        # Return original df with new sentiment attached\n",
    "        return pd.concat([df, sentiment_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vaderSentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-b779408544e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vaderSentiment' is not defined"
     ]
    }
   ],
   "source": [
    "vaderSentiment.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_entity_sentiment(df,\n",
    "                              textacy_col,\n",
    "                              entity,\n",
    "                              inplace=True,\n",
    "                              vader_sent_types=['neg', 'neu', 'pos', 'compound'],\n",
    "                              keep_stats=['count', 'mean', 'min', '25%', '50%', '75%', 'max']):\n",
    "    \"\"\"\n",
    "    Pull the descriptive sentiment stats of text sentence with a specified entity in it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataframe which holds the text\n",
    "    textacy_col : str\n",
    "        The name to give to the column with the textacy doc objects\n",
    "    entity : str\n",
    "        The entity to search the textacy Doc object for\n",
    "    inplace : bool\n",
    "        Whether to return the entire df with the sentiment info or the sentiment info alone\n",
    "        Default is False\n",
    "    vader_sent_types : list\n",
    "        The type of sentiment to extract. neg: negative, pos: positive, neu: neutral, compound is \n",
    "        comination of all three types of all \n",
    "    keep_stats : list\n",
    "        A list of the summary statistics to keep. Default is all returned by pandas DataFrame.describe() method\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Either the dataframe passed as arg with the sentiment info as trailing columns\n",
    "        or the sentiment descriptive stats by itself\n",
    "    \"\"\"\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_rows = []\n",
    "    for text in df[textacy_col].values:\n",
    "        text_entities = list(entity_statements(text, entity)) \n",
    "\n",
    "\n",
    "         # Iterate through all sentences and get sentiment analysis\n",
    "        entity_sentiment_info = [vader_analyzer.polarity_scores(sentence)\n",
    "                                for\n",
    "                                sentence\n",
    "                                in\n",
    "                                text_entities]\n",
    "\n",
    "        # After taking sentiments, turn into a dataframe and describe\n",
    "        try:\n",
    "            # Indices and columns to keep\n",
    "            keep_stats = keep_stats\n",
    "            keep_cols = sentiment_types\n",
    "\n",
    "            # Describe those columns\n",
    "            summary_stats = pd.DataFrame(entity_sentiment_info).describe().loc[keep_stats, keep_cols]\n",
    "\n",
    "            # Add row to list\n",
    "            sentiment_rows.append(pivot_df_to_row(summary_stats))\n",
    "\n",
    "        # If there's nothing to describe\n",
    "        except ValueError as e:\n",
    "            # Create a summary stats with nulls\n",
    "            summary_stats = pd.DataFrame(index=keep_stats, columns=keep_cols)\n",
    "\n",
    "            # Add to list of rows\n",
    "            sentiment_rows.append(pivot_df_to_row(summary_stats))\n",
    "    # Concatenate All rows together into one dataframe\n",
    "    sentiment_df = pd.concat(sentiment_rows).add_prefix(entity+'_')\n",
    "\n",
    "    if not inplace:\n",
    "        return sentiment_df.reset_index(drop=True)\n",
    "    else:\n",
    "        # Return original df with new sentiment attached\n",
    "        return pd.concat([df, sentiment_df], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# sentiment_rows = []\n",
    "# for text in df[textacy_col].values:\n",
    "#     text_entities = list(entity_statements(text, entity)) \n",
    "    \n",
    "\n",
    "#      # Iterate through all sentences and get sentiment analysis\n",
    "#     entity_sentiment_info = [vader_analyzer.polarity_scores(sentence)\n",
    "#                             for\n",
    "#                             sentence\n",
    "#                             in\n",
    "#                             text_entities]\n",
    "\n",
    "#     # After taking sentiments, turn into a dataframe and describe\n",
    "#     try:\n",
    "#         # Indices and columns to keep\n",
    "#         keep_stats = ['count', 'mean', 'min', 'max']\n",
    "#         keep_cols = ['neg', 'neu', 'pos', 'compound'']\n",
    "\n",
    "#         # Describe those columns\n",
    "#         summary_stats = pd.DataFrame(entity_sentiment_info).describe().loc[keep_stats, keep_cols]\n",
    "\n",
    "#         # Add row to list\n",
    "#         sentiment_rows.append(pivot_df_to_row(summary_stats))\n",
    "\n",
    "#     # If there's nothing to describe\n",
    "#     except ValueError as e:\n",
    "#         # Create a summary stats with nulls\n",
    "#         summary_stats = pd.DataFrame(index=keep_stats, columns=keep_cols)\n",
    "\n",
    "#         # Add to list of rows\n",
    "#         sentiment_rows.append(pivot_df_to_row(summary_stats))\n",
    "# # Concatenate All rows together into one dataframe\n",
    "# sentiment_df = pd.concat(sentiment_rows).add_prefix(entity+'_')\n",
    "\n",
    "# if not inplace:\n",
    "#     return sentiment_df.reset_index(drop=True)\n",
    "# else:\n",
    "#     # Return original df with new sentiment attached\n",
    "#     return pd.concat([df, sentiment_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vaderSentiment' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-4b7f787dfc28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'vaderSentiment' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import vaderSentiment\n",
    "vaderSentiment.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A very, very, very slow-moving, aimless movie about a distressed, drifting young man.   {'neg': 0.219, 'neu': 0.781, 'pos': 0.0, 'compound': -0.4215}\n",
      "Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.   {'neg': 0.222, 'neu': 0.778, 'pos': 0.0, 'compound': -0.5507}\n",
      "Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.   {'neg': 0.25, 'neu': 0.667, 'pos': 0.083, 'compound': -0.7178}\n",
      "Very little music or anything to speak of.  --------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.   {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'compound': 0.6369}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for sentence in df.text.values[:5]:\n",
    "    vs = vader_analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Entity Text, Counts and Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each entity selected, return the count of entity occurence as well as mean, min and max of sentiments of sentences that contain said entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.convenience_functions.pandas_functions import pivot_df_to_row\n",
    "from src.convenience_functions.pandas_functions import null_column_report\n",
    "sents = vader_entity_sentiment(df=df,\n",
    "                       textacy_col='textacy_doc', \n",
    "                       entity='characters', \n",
    "                       inplace=False,\n",
    "                       sentiment_types = ['neg', 'neu', 'pos', 'compound'],\n",
    "                       keep_stats=cfg['sentiment_descriptive_stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:\n",
      "characters_neg_count\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neg_mean\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neg_min\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neg_25%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neg_50%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neg_75%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neg_max\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neu_count\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neu_mean\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neu_min\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neu_25%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neu_50%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neu_75%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_neu_max\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_pos_count\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_pos_mean\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_pos_min\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_pos_25%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_pos_50%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_pos_75%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_pos_max\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_compound_count\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_compound_mean\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_compound_min\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_compound_25%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_compound_50%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_compound_75%\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n",
      "Column:\n",
      "characters_compound_max\n",
      "Total Nulls:\n",
      "717\n",
      "Percent Null:\n",
      "0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_column_report(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Extracting the following descriptive stats for entity sentiments: {} \"\"\".format(cfg['sentiment_descriptive_stats']))\n",
    "\n",
    "logging.info(\"\"\"Extracting the sentiments for the following entities: {} \"\"\".format(cfg['entities']))\n",
    "\n",
    "sentiments = [vader_entity_sentiment(df=df, \n",
    "                                        textacy_col='textacy_doc', \n",
    "                                        entity=entity, \n",
    "                                        inplace=False,\n",
    "                                        sentiment_types = ['neg', 'neu', 'pos', 'compound'],\n",
    "                                        keep_stats=cfg['sentiment_descriptive_stats']) \n",
    "              for entity\n",
    "              in cfg['entities']]\n",
    "# Concat to single df\n",
    "sentiments = pd.concat(sentiments, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat sentiment features and original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_with_sentiment_info = pd.concat([df, sentiments], axis=1).drop(labels=['textacy_doc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'sentiment_label', 'characters_neg_count',\n",
       "       'characters_neg_mean', 'characters_neg_min', 'characters_neg_25%',\n",
       "       'characters_neg_50%', 'characters_neg_75%', 'characters_neg_max',\n",
       "       'characters_neu_count',\n",
       "       ...\n",
       "       'villain_pos_50%', 'villain_pos_75%', 'villain_pos_max',\n",
       "       'villain_compound_count', 'villain_compound_mean',\n",
       "       'villain_compound_min', 'villain_compound_25%', 'villain_compound_50%',\n",
       "       'villain_compound_75%', 'villain_compound_max'],\n",
       "      dtype='object', length=114)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_sentiment_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_with_sentiment_info.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\")\n",
    "archive_output_path = 'output/{}.csv'.format(now)\n",
    "logging.info(\"\"\"Outputting sentiments to {}\"\"\".format(archive_output_path))\n",
    "texts_with_sentiment_info.to_csv(archive_output_path, index=False)\n",
    "print(\"\"\"Outputting sentiments to {}\"\"\".format(archive_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
