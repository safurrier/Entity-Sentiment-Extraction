{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_CONFIG_ERR_MSG = \"\"\"No config file found. Root directory is determined by presence of \"config.yaml\" file.\"\"\"        \n",
    "\n",
    "original_wd = os.getcwd()\n",
    "\n",
    "# Number of times to move back in directory\n",
    "num_retries = 10\n",
    "for x in range(0, num_retries):\n",
    "    # try to load config file    \n",
    "    try:\n",
    "        with open(\"config.yaml\", 'r') as stream:\n",
    "            cfg = yaml.safe_load(stream)\n",
    "    # If not found move back one directory level\n",
    "    except FileNotFoundError:\n",
    "        os.chdir('../')\n",
    "        # If reached the max number of directory levels change to original wd and print error msg\n",
    "        if x+1 == num_retries:\n",
    "            os.chdir(original_wd)\n",
    "            print(NO_CONFIG_ERR_MSG)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import local code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Add current wd to path for localimports\n",
    "path = os.getcwd()\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path) \n",
    "\n",
    "from src.convenience_functions.textacy_convenience_functions import load_textacy_corpus\n",
    "from src.convenience_functions.textacy_convenience_functions import entity_statements\n",
    "from src.convenience_functions.textacy_convenience_functions import list_of_entity_statements\n",
    "from src.convenience_functions.textacy_convenience_functions import dask_df_apply\n",
    "from src.textblob_entity_sentiment import textblob_entity_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\")\n",
    "logging.basicConfig(filename='logs/{}.txt'.format(now), \n",
    "                    level=logging.INFO,\n",
    "                    filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Reading in data from {}\"\"\".format(cfg['input_filepath']))\n",
    "\n",
    "\n",
    "df = pd.read_csv(cfg['input_filepath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Multiprocessing of applied textacy docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dask to multiprocess the loading of textacy docs for each text\n",
    "\n",
    "1. Use dask to create partitioned dataframe\n",
    "\n",
    "2. To each partition map an apply that creates textacy docs from the Policy_Text column\n",
    "\n",
    "3. Concatenate back to original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Creating textacy Doc objects using the text found in the '{}' column\"\"\".format(cfg['text_col']))\n",
    "\n",
    "df = dask_df_apply(df, cfg['text_col'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Entity Text, Counts and Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each entity selected, return the count of entity occurence as well as mean, min and max of sentiments of sentences that contain said entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Extracting the following descriptive stats for entity sentiments: {} \"\"\".format(cfg['sentiment_descriptive_stats']))\n",
    "\n",
    "logging.info(\"\"\"Extracting the sentiments for the following entities: {} \"\"\".format(cfg['entities']))\n",
    "\n",
    "sentiments = [textblob_entity_sentiment(df=df, \n",
    "                                        textacy_col='textacy_doc', \n",
    "                                        entity=entity, \n",
    "                                        inplace=False,\n",
    "                                        keep_stats=cfg['sentiment_descriptive_stats']) \n",
    "              for entity\n",
    "              in cfg['entities']]\n",
    "# Concat to single df\n",
    "sentiments = pd.concat(sentiments, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat sentiment features and original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_with_sentiment_info = pd.concat([df, sentiments], axis=1).drop(labels=['textacy_doc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'sentiment_label', 'characters_polarity_count',\n",
       "       'characters_polarity_mean', 'characters_polarity_min',\n",
       "       'characters_polarity_25%', 'characters_polarity_50%',\n",
       "       'characters_polarity_75%', 'characters_polarity_max',\n",
       "       'plot_polarity_count', 'plot_polarity_mean', 'plot_polarity_min',\n",
       "       'plot_polarity_25%', 'plot_polarity_50%', 'plot_polarity_75%',\n",
       "       'plot_polarity_max', 'hero_polarity_count', 'hero_polarity_mean',\n",
       "       'hero_polarity_min', 'hero_polarity_25%', 'hero_polarity_50%',\n",
       "       'hero_polarity_75%', 'hero_polarity_max', 'villain_polarity_count',\n",
       "       'villain_polarity_mean', 'villain_polarity_min', 'villain_polarity_25%',\n",
       "       'villain_polarity_50%', 'villain_polarity_75%', 'villain_polarity_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_sentiment_info.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\")\n",
    "archive_output_path = 'output/{}.csv'.format(now)\n",
    "logging.info(\"\"\"Outputting sentiments to {}\"\"\".format(archive_output_path))\n",
    "texts_with_sentiment_info.to_csv(archive_output_path, index=False)\n",
    "print(\"\"\"Outputting sentiments to {}\"\"\".format(archive_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
